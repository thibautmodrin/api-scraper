# ğŸš€ Scraper API Freelance â€“ Free-Work

Une API FastAPI qui dÃ©clenche un scraping quotidien de la plateforme [Free-Work](https://www.free-work.com/fr/tech-it/jobs), stocke les offres dans une base PostgreSQL, et expose les rÃ©sultats via une API REST.

---

## ğŸ“¦ FonctionnalitÃ©s

- ğŸ” Scraping des offres de missions via `Scrapy` Ã  partir d'un mot-clÃ©
- ğŸ§  DÃ©tection des offres dÃ©jÃ  existantes pour Ã©viter les doublons
- ğŸ—“ï¸ Historique des scrapes par date et mot-clÃ© (table `scraping_logs`)
- ğŸŒ API REST pour consulter les offres (`/annonces/`) et lancer un scraping (`/run-scraper/`)
- ğŸ’¾ Stockage PostgreSQL (local ou distant via Render)
- ğŸ“Š IntÃ©gration possible avec Metabase, NocoDB, etc.

---

## ğŸ› ï¸ Stack Technique

- `FastAPI` â€“ API REST
- `Scrapy` â€“ Web scraping
- `PostgreSQL` â€“ Base de donnÃ©es relationnelle
- `Docker` / `Docker Compose` â€“ Conteneurisation
- `Render.com` â€“ HÃ©bergement PostgreSQL ou API
- `NocoDB`, `Metabase` â€“ (optionnel) Visualisation no-code

---

## ğŸ§± Structure du projet

```
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ main.py # Lâ€™API FastAPI (endpoints)
â”‚ â”œâ”€â”€ scraper.py # Spider Scrapy pour Free-Work
â”‚ â”œâ”€â”€ database.py # Connexion et logique PostgreSQL
â”‚ â”œâ”€â”€ Dockerfile # Dockerisation de lâ€™API
â”œâ”€â”€ README.md # Ce fichier
â”œâ”€â”€ .env # Variables dâ€™environnement (DATABASE_URL)
```
---

## âš™ï¸ Configuration

### ğŸ” Variables dâ€™environnement

CrÃ©er un fichier `.env` (non versionnÃ©) :

```env
DATABASE_URL=postgresql://<user>:<password>@<host>/<dbname>
```

â–¶ï¸ Lancer en local
1. Cloner le projet
git clone <lien_du_repo>
cd scraper_api

3. Installer les dÃ©pendances
pip install -r requirements.txt

5. Lancer lâ€™API
uvicorn main:app --reload


ğŸ§ª Endpoints disponibles

MÃ©thode	URL	Description
GET	/annonces/	RÃ©cupÃ¨re toutes les offres en BDD
POST	/run-scraper/?keyword=data%20engineer	Lance le scraping pour un mot-clÃ© donnÃ©

ğŸ•’ DÃ©clencher automatiquement chaque jour (cron)
Linux crontab -e :
0 9 * * * curl -X POST "https://api-scraper-6js4.onrender.com/run-scraper/?keyword=data%20engineer"

ğŸ³ Docker
Lancer avec Docker
docker build -t scraper-api .
docker run -p 8000:8000 -e DATABASE_URL=postgresql://... scraper-api

ğŸ—ƒï¸ Base de donnÃ©es (tables attendues)
```
-- Table des offres
CREATE TABLE offres (
  id SERIAL PRIMARY KEY,
  type TEXT,
  titre TEXT,
  entreprise TEXT,
  technos TEXT[],
  duree TEXT,
  salaire TEXT,
  tjm TEXT,
  teletravail TEXT,
  lieu TEXT,
  date_extraction DATE,
  keyword TEXT
);

-- Table des logs de scraping
CREATE TABLE scraping_logs (
  id SERIAL PRIMARY KEY,
  keyword TEXT,
  date DATE
);

```

ğŸ“ˆ Visualisation No-code (optionnel)
Tu peux connecter ta base PostgreSQL Ã  :

Metabase

NocoDB

Budibase

ğŸ“„ Licence
Ce projet est sous licence MIT.

ğŸ™Œ Auteur
DÃ©veloppÃ© par @burgovida21 â€” Data Engineer / Freelance / Android Developer.



Souhaites-tu que je te le mette directement dans un fichier `README.md` prÃªt Ã  tÃ©lÃ©charger ?
