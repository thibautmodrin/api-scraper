# ğŸš€ Scraper API Freelance â€“ Free-Work

Une API FastAPI qui dÃ©clenche un scraping quotidien de la plateforme [Free-Work](https://www.free-work.com/fr/tech-it/jobs), stocke les offres dans une base PostgreSQL, et expose les rÃ©sultats via une API REST.

---

## ğŸ“¦ FonctionnalitÃ©s

- ğŸ” Scraping des offres de missions via `Scrapy` Ã  partir d'un mot-clÃ©
- ğŸ§  DÃ©tection des offres dÃ©jÃ  existantes pour Ã©viter les doublons
- ğŸ—“ï¸ Historique des scrapes par date et mot-clÃ© (table `scraping_logs`)
- ğŸŒ API REST pour consulter les offres (`/annonces/`) et lancer un scraping (`/run-scraper/`)
- ğŸ’¾ Stockage PostgreSQL (local ou distant via Render)
- ğŸ“Š IntÃ©gration possible avec Metabase, NocoDB, etc.

---

## ğŸ› ï¸ Stack Technique

- `FastAPI` â€“ API REST
- `Scrapy` â€“ Web scraping
- `PostgreSQL` â€“ Base de donnÃ©es relationnelle
- `Docker` / `Docker Compose` â€“ Conteneurisation
- `Render.com` â€“ HÃ©bergement PostgreSQL ou API
- `NocoDB`, `Metabase` â€“ (optionnel) Visualisation no-code

---

## ğŸ§± Structure du projet

```bash
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py            # Lâ€™API FastAPI (endpoints)
â”‚   â”œâ”€â”€ scraper.py         # Spider Scrapy pour Free-Work
â”‚   â”œâ”€â”€ database.py        # Connexion et logique PostgreSQL
â”‚   â”œâ”€â”€ Dockerfile         # Dockerisation de lâ€™API
â”œâ”€â”€ README.md              # Ce fichier
â”œâ”€â”€ .env                   # Variables dâ€™environnement (DATABASE_URL)
```

---

## âš™ï¸ Configuration

### ğŸ” Variables dâ€™environnement

CrÃ©er un fichier `.env` :

```env
DATABASE_URL=postgresql://<user>:<password>@<host>/<dbname>
```

---

## â–¶ï¸ Lancer en local

1. **Cloner le projet**

```bash
git clone <lien_du_repo>
cd scraper_api
```

2. **Installer les dÃ©pendances**

```bash
pip install -r requirements.txt
```

3. **Lancer lâ€™API**

```bash
uvicorn main:app --reload
```

---

## ğŸ§ª Endpoints disponibles

| MÃ©thode | URL                                                   | Description                              |
|---------|--------------------------------------------------------|------------------------------------------|
| GET     | `/annonces/`                                          | RÃ©cupÃ¨re toutes les offres en BDD        |
| POST    | `/run-scraper/?keyword=data%20engineer`               | Lance le scraping pour un mot-clÃ© donnÃ©  |

---

## ğŸ•’ DÃ©clenchement automatique (cron)

Ajoute cette ligne Ã  ton `crontab` (`crontab -e`) pour dÃ©clencher tous les jours Ã  9h :

```bash
0 9 * * * curl -X POST "https://api-scraper-6js4.onrender.com/run-scraper/?keyword=data%20engineer"
```

---

## ğŸ³ Lancer avec Docker

```bash
docker build -t scraper-api .
docker run -p 8000:8000 -e DATABASE_URL=postgresql://... scraper-api
```

---

## ğŸ—ƒï¸ Base de donnÃ©es (tables attendues)

```sql
-- Table des offres
CREATE TABLE offres (
  id SERIAL PRIMARY KEY,
  type TEXT,
  titre TEXT,
  entreprise TEXT,
  technos TEXT[],
  duree TEXT,
  salaire TEXT,
  tjm TEXT,
  teletravail TEXT,
  lieu TEXT,
  date_extraction DATE,
  keyword TEXT
);

-- Table des logs de scraping
CREATE TABLE scraping_logs (
  id SERIAL PRIMARY KEY,
  keyword TEXT,
  date DATE
);
```

---

## ğŸ“ˆ Visualisation No-code (optionnel)

Tu peux connecter ta base PostgreSQL Ã  des outils comme :

- [Metabase](https://www.metabase.com/)
- [NocoDB](https://www.nocodb.com/)


---

## ğŸ“„ Licence

Ce projet est sous licence **MIT**.

---

## ğŸ™Œ Auteur

DÃ©veloppÃ© par **[@burgovida21](https://github.com/ton-profil)**  
Data Engineer / Freelance / Android Developer.
```

---

Souhaites-tu que je tâ€™envoie ce contenu dans un fichier `README.md` Ã  tÃ©lÃ©charger directement ?
