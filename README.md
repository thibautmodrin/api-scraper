

# ğŸš€ Scraper API Freelance â€“ Plateforme de missions

Une API FastAPI qui dÃ©clenche un scraping quotidien dâ€™une plateforme publique de missions freelance, stocke les offres dans une base PostgreSQL, et expose les rÃ©sultats via une API REST.

---

## ğŸ“¦ FonctionnalitÃ©s

* ğŸ” Scraping des offres via `Scrapy` Ã  partir dâ€™un mot-clÃ©
* ğŸ§  DÃ©tection des doublons pour Ã©viter les rÃ©pÃ©titions
* ğŸ—“ï¸ Historique des extractions (par date et mot-clÃ©)
* ğŸŒ API REST pour consulter les offres (`/annonces/`) et dÃ©clencher un scraping (`/run-scraper/`)
* ğŸ’¾ Stockage PostgreSQL (local ou distant)
* ğŸ“Š IntÃ©gration possible avec Metabase, NocoDB, etc.

---

## ğŸ› ï¸ Stack Technique

* `FastAPI` â€“ API REST
* `Scrapy` â€“ Web scraping
* `PostgreSQL` â€“ Base de donnÃ©es relationnelle
* `Docker` / `Docker Compose` â€“ Conteneurisation
* `Render.com` â€“ HÃ©bergement
* `Metabase`, `NocoDB` â€“ (optionnel) Visualisation no-code

---

## ğŸ§± Structure du projet

```bash
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py            # Lâ€™API FastAPI (endpoints)
â”‚   â”œâ”€â”€ scraper.py         # Spider Scrapy
â”‚   â”œâ”€â”€ database.py        # Connexion et logique PostgreSQL
â”‚   â”œâ”€â”€ Dockerfile         # Dockerisation de lâ€™API
â”œâ”€â”€ README.md              # Ce fichier
â”œâ”€â”€ .env                   # Variables dâ€™environnement (DATABASE_URL)
```

---

## âš™ï¸ Configuration

### ğŸ” Variables dâ€™environnement

CrÃ©er un fichier `.env` :

```env
DATABASE_URL=postgresql://<user>:<password>@<host>/<dbname>
```

---

## â–¶ï¸ Lancer en local

1. **Cloner le projet**

```bash
git clone <lien_du_repo>
cd scraper_api
```

2. **Installer les dÃ©pendances**

```bash
pip install -r requirements.txt
```

3. **Lancer lâ€™API**

```bash
uvicorn main:app --reload
```

---

## ğŸ§ª Endpoints disponibles

| MÃ©thode | URL                                     | Description                             |
| ------- | --------------------------------------- | --------------------------------------- |
| GET     | `/annonces/`                            | RÃ©cupÃ¨re toutes les offres en BDD       |
| POST    | `/run-scraper/?keyword=data%20engineer` | Lance le scraping pour un mot-clÃ© donnÃ© |

---

## ğŸ•’ DÃ©clenchement automatique (cron)

Ajoute cette ligne Ã  ton `crontab` (`crontab -e`) pour dÃ©clencher tous les jours Ã  9h :

```bash
0 9 * * * curl -X POST "https://api-scraper-6js4.onrender.com/run-scraper/?keyword=data%20engineer"
```

---

## ğŸ³ Lancer avec Docker

```bash
docker build -t scraper-api .
docker run -p 8000:8000 -e DATABASE_URL=postgresql://... scraper-api
```

---

## ğŸ—ƒï¸ Base de donnÃ©es (tables attendues)

```sql
-- Table des offres
CREATE TABLE offres (
  id SERIAL PRIMARY KEY,
  type TEXT,
  titre TEXT,
  entreprise TEXT,
  technos TEXT[],
  duree TEXT,
  salaire TEXT,
  tjm TEXT,
  teletravail TEXT,
  lieu TEXT,
  date_extraction DATE,
  keyword TEXT
);

-- Table des logs de scraping
CREATE TABLE scraping_logs (
  id SERIAL PRIMARY KEY,
  keyword TEXT,
  date DATE
);
```

---

## ğŸ“ˆ Visualisation No-code (optionnel)

Tu peux connecter ta base PostgreSQL Ã  des outils comme :

* [Metabase](https://www.metabase.com/)
* [NocoDB](https://www.nocodb.com/)

---

## ğŸ“„ Licence

Ce projet est sous licence **MIT**.


